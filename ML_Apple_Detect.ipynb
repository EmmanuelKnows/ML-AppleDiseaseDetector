{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oNf41mzpzDWQ",
        "NeJhTWW1zjw7",
        "dqhksSjD0EIa",
        "arwzEvIf0Rgi",
        "kmr0WfMy0bh9",
        "726cc532",
        "1dae5f53",
        "216b84c7",
        "FRIXNDBe60cM",
        "f6eef5b4",
        "93415226",
        "CNxR_LV29fbu",
        "SGVBwONQ9t85"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPfpWsF8m1NP36xC1asTVqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmanuelKnows/ML-AppleDiseaseDetector/blob/main/ML_Apple_Detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning - Apple Disease Detection\n",
        "### Model development\n",
        "\n",
        "This project implements a deep learning model using transfer learning with MobileNetV2 to detect diseases in apple fruits. The model is trained on a dataset sourced from Kaggle and utilizes MLflow for experiment tracking and model management."
      ],
      "metadata": {
        "id": "lC9SJ7jvycqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project Overview\n",
        "\n",
        "The goal of this project is to build a robust image classification model that can accurately identify different diseases affecting apples based on images. This can be valuable for agricultural applications, enabling early detection and management of diseases.\n"
      ],
      "metadata": {
        "id": "B-SHti8Z_2hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Features\n",
        "\n",
        "- **Transfer Learning:** Leverages the power of a pre-trained convolutional neural network (MobileNetV2) to benefit from learned features on a large dataset (ImageNet).\n",
        "- **Data Augmentation:** Techniques like rotation, zooming, and flipping are applied to the training data to increase the size and diversity of the dataset, improving the model's ability to generalize.\n",
        "- **MLflow Integration:** Tracks various aspects of the machine learning lifecycle, including parameters, metrics, and artifacts (model weights, plots, reports). This facilitates experiment comparison and reproducibility.\n",
        "- **Callbacks:** Utilizes Keras callbacks such as ModelCheckpoint (to save the best model), EarlyStopping (to prevent overfitting), and ReduceLROnPlateau (to adjust the learning rate during training)."
      ],
      "metadata": {
        "id": "oEifVJgS_9e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset from Kaggle\n",
        "#### Code cell explaining how to download the dataset using `kagglehub`."
      ],
      "metadata": {
        "id": "oNf41mzpzDWQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcL5Rm4dQGU3"
      },
      "outputs": [],
      "source": [
        "#import kagglehub\n",
        "# link: https://www.kaggle.com/datasets/ateebnoone/fruits-dataset-for-fruit-disease-classification\n",
        "\n",
        "# Download latest version\n",
        "#path = kagglehub.dataset_download(\"ateebnoone/fruits-dataset-for-fruit-disease-classification\")\n",
        "\n",
        "#print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the `mlflow` library."
      ],
      "metadata": {
        "id": "NeJhTWW1zjw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mlflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmtgzmnJDgeo",
        "outputId": "b2399c03-9a81-4734-d2a8-547923f5dd63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.4.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting mlflow-skinny==3.4.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.4.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.4.0 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.5)\n",
            "Requirement already satisfied: cryptography<46,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fastmcp<3,>=2.0.0 (from mlflow)\n",
            "  Downloading fastmcp-2.12.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<22,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.2)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.43)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.4.0->mlflow)\n",
            "  Downloading databricks_sdk-0.67.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.116.2)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.4.0->mlflow)\n",
            "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (2.11.9)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.1.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<46,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: authlib>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.6.4)\n",
            "Collecting cyclopts>=3.0.0 (from fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading cyclopts-3.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting exceptiongroup>=1.2.2 (from fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (0.28.1)\n",
            "Requirement already satisfied: mcp<2.0.0,>=1.12.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.14.1)\n",
            "Collecting openapi-core>=0.19.5 (from fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading openapi_core-0.19.5-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting openapi-pydantic>=0.5.1 (from fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.10.0)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (13.9.4)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<46,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (0.17.0)\n",
            "Collecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading rich_rst-1.3.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.4.0->mlflow) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.4.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (3.0.2)\n",
            "Collecting isodate (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting jsonschema-path<0.4.0,>=0.3.1 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (10.8.0)\n",
            "Collecting openapi-schema-validator<0.7.0,>=0.6.0 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading openapi_schema_validator-0.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting openapi-spec-validator<0.8.0,>=0.7.1 (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading openapi_spec_validator-0.7.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting parse (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
            "  Downloading werkzeug-3.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.4.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.4.1)\n",
            "Collecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (2.19.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.3.1)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.27.1)\n",
            "Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (0.1.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.1.4)\n",
            "Collecting lazy-object-proxy<2.0.0,>=1.7.1 (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow)\n",
            "  Downloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.12/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (0.21.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.4.0-py3-none-any.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.4.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.4.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastmcp-2.12.3-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cyclopts-3.24.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.67.0-py3-none-any.whl (718 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.4/718.4 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading openapi_core-0.19.5-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
            "Downloading jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\n",
            "Downloading openapi_schema_validator-0.6.3-py3-none-any.whl (8.8 kB)\n",
            "Downloading openapi_spec_validator-0.7.2-py3-none-any.whl (39 kB)\n",
            "Downloading rich_rst-1.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lazy_object_proxy-1.12.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathable-0.4.4-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: parse, werkzeug, pathable, opentelemetry-proto, lazy-object-proxy, isodate, gunicorn, graphql-core, exceptiongroup, dnspython, jsonschema-path, graphql-relay, email-validator, docker, rich-rst, openapi-pydantic, graphene, databricks-sdk, openapi-schema-validator, cyclopts, openapi-spec-validator, mlflow-tracing, mlflow-skinny, openapi-core, fastmcp, mlflow\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "Successfully installed cyclopts-3.24.0 databricks-sdk-0.67.0 dnspython-2.8.0 docker-7.1.0 email-validator-2.3.0 exceptiongroup-1.3.0 fastmcp-2.12.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 isodate-0.7.2 jsonschema-path-0.3.4 lazy-object-proxy-1.12.0 mlflow-3.4.0 mlflow-skinny-3.4.0 mlflow-tracing-3.4.0 openapi-core-0.19.5 openapi-pydantic-0.5.1 openapi-schema-validator-0.6.3 openapi-spec-validator-0.7.2 opentelemetry-proto-1.37.0 parse-1.20.2 pathable-0.4.4 rich-rst-1.3.1 werkzeug-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries and dependencies\n",
        "\n",
        "libraries and dependencies for building and training the model, including TensorFlow, Keras, Matplotlib, NumPy, scikit-learn, seaborn, and MLflow."
      ],
      "metadata": {
        "id": "dqhksSjD0EIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries and Dependencies\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.applications import MobileNetV2 # Or VGG16, ResNet50, etc.\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import mlflow\n",
        "import mlflow.keras # Import mlflow.keras for autologging"
      ],
      "metadata": {
        "id": "5QiPW2cXxVS0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive to access the dataset stored there."
      ],
      "metadata": {
        "id": "arwzEvIf0Rgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sqsBfWwy60P",
        "outputId": "f364ae65-7f96-4b73-bd94-29bccc63f3f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image preprocessing and data augmentation\n",
        "\n",
        "using `ImageDataGenerator`, setting up training, validation, and test data generators from the dataset directories. It also prints the class names and number of classes detected."
      ],
      "metadata": {
        "id": "kmr0WfMy0bh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Acquisition (Getting my data images in folders: 'train/healthy', 'train/scab', etc.)\n",
        "\n",
        "\n",
        "# 2. Image Preprocessing and Augmentation (using Keras ImageDataGenerator)\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image parameters\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create ImageDataGenerators\n",
        "# For training, include data augmentation and rescale pixel values\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,             # Normalize pixel values to [0, 1]\n",
        "    shear_range=0.2,            # Shear transformations\n",
        "    zoom_range=0.2,             # Random zoom\n",
        "    horizontal_flip=True,       # Random horizontal flips\n",
        "    validation_split=0.2        # Split a portion for validation\n",
        ")\n",
        "\n",
        "# For testing, only rescale pixel values (no augmentation)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load data from directories using flow_from_directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Datasets/apple-fruit-ds/Train',        # Path to the training directory\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',   # For multi-class classification (one-hot encoding)\n",
        "    subset='training',          # Specify this is the training subset\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Datasets/apple-fruit-ds/Train',        # Same path as training, but for validation subset\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',        # Specify this is the validation subset\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Datasets/apple-fruit-ds/Test',         # Path to the test directory\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False               # Important: Do not shuffle test data for consistent evaluation\n",
        ")\n",
        "\n",
        "\n",
        "# Get class names and number of classes\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "num_classes = len(class_names)\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# You can get the first batch and inspect shapes\n",
        "# images, labels = next(train_generator)\n",
        "# print(f\"Image batch shape: {images.shape}\")\n",
        "# print(f\"Labels batch shape: {labels.shape}\")\n",
        "\n",
        "# Now `train_generator`, `validation_generator`, `test_generator` are ready for model.fit()\n",
        "# e.g., model.fit(train_generator, validation_data=validation_generator, epochs=..., steps_per_epoch=train_generator.samples // BATCH_SIZE)"
      ],
      "metadata": {
        "id": "uk5h6k9H33Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "726cc532"
      },
      "source": [
        "### Setting up MLflow Tracking Server in Colab\n",
        "\n",
        "**Start the Tracking Server:** Run the following command in a code cell. This code will start a server in the background.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f5af521"
      },
      "source": [
        "!mlflow ui &>/dev/null &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dae5f53"
      },
      "source": [
        "### Install the `ngrok` library for creating a tunnel to access the MLflow UI.\n",
        "\n",
        "**Get the Ngrok Tunnel URL:** Since the server is running on `localhost` inside the Colab environment, you'll need a tunnel to access the UI from your browser. We'll use `ngrok` for this. First, install `ngrok`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de5d282a"
      },
      "source": [
        "!pip install ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216b84c7"
      },
      "source": [
        "### Authenticate ngrok using a token stored in Colab secrets.\n",
        "\n",
        "**Authenticate ngrok (Optional but Recommended):** If you have an ngrok account, you can add your authtoken for more reliable connections. Replace `YOUR_AUTHTOKEN` with your actual token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73c64f9c",
        "cellView": "form"
      },
      "source": [
        "# @title Store your ngrok authtoken securely\n",
        "from google.colab import userdata\n",
        "\n",
        "# Input your ngrok authtoken\n",
        "# To get an authtoken, go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# Use the Colab Secrets Manager to store your token securely\n",
        "# the secret is named 'NGROK_AUTHTOKEN'\n",
        "ngrok_authtoken = userdata.get('NGROK_AUTHTOKEN') # get the secret key from colab secret\n",
        "\n",
        "# Or uncomment the line below and paste your token directly (less secure)\n",
        "# ngrok_authtoken = \"Your Authtoken\"\n",
        "\n",
        "# for authtoken stored in Colab secrets, use the line below\n",
        "!ngrok config add-authtoken $ngrok_authtoken"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea8a6276"
      },
      "source": [
        "**Create the ngrok Tunnel:** Run this command to create a tunnel to the MLflow server running on port 5000."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyngrok"
      ],
      "metadata": {
        "id": "kWaoINfvHbpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an ngrok tunnel to the MLflow server\n",
        "Running on port 5000 and print the public URL to access the MLflow UI."
      ],
      "metadata": {
        "id": "FRIXNDBe60cM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81d69153"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Terminate any existing ngrok tunnels\n",
        "# ngrok.kill()\n",
        "\n",
        "# Get the authtoken from Colab secrets\n",
        "NGROK_AUTHTOKEN = userdata.get('NGROK_AUTHTOKEN')\n",
        "if NGROK_AUTHTOKEN:\n",
        "  ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "else:\n",
        "  print(\"Ngrok authtoken not found in Colab secrets. You may experience connection issues.\")\n",
        "  print(\"Add NGROK_AUTHTOKEN to Colab secrets (left panel, '🔑')\")\n",
        "\n",
        "# Open a ngrok tunnel to the MLflow server (port 5000)\n",
        "print(\"Opening ngrok tunnel...\")\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(f\"MLflow UI Tunnel URL: {public_url}\")\n",
        "\n",
        "# You can now access the MLflow UI at the URL printed above.\n",
        "# Use this URL in your browser to view tracking information."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6eef5b4"
      },
      "source": [
        "### Set the MLflow tracking URI to the public URL provided by ngrok and enable Keras autologging again.\n",
        "\n",
        "**Set the MLflow Tracking URI:** Now that you have the public URL, set the MLflow tracking URI in your code to point to this URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d4e27a0"
      },
      "source": [
        "# You can use the `public_url` variable from the previous cell\n",
        "# Or replace with the actual URL printed by ngrok\n",
        "mlflow.set_tracking_uri(public_url)\n",
        "mlflow.set_experiment(\"Apple_Disease_Detection\")\n",
        "\n",
        "# Enable Keras autologging\n",
        "mlflow.keras.autolog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93415226"
      },
      "source": [
        "### Define and compile the deep learning model\n",
        "\n",
        "using transfer learning with MobileNetV2 as the base model and adding custom classification layers. It also prints the model summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume these generators are already defined from your previous step:\n",
        "# train_generator\n",
        "# validation_generator\n",
        "# test_generator\n",
        "\n",
        "# Get the number of classes from your generators\n",
        "num_classes = train_generator.num_classes\n",
        "print(f\"Number of classes detected: {num_classes}\")\n",
        "print(f\"Class indices: {train_generator.class_indices}\")\n",
        "\n",
        "# Define input image dimensions (should match target_size in ImageDataGenerator)\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3) # 3 for RGB channels\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model\n",
        "# include_top=False means we don't include the classification layers of MobileNetV2\n",
        "# weights='imagenet' means it's pre-trained on the ImageNet dataset\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
        "\n",
        "# Freeze the convolutional layers of the base model\n",
        "# This prevents their weights from being updated during initial training\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x) # Converts feature maps to a single vector per image\n",
        "x = Dense(256, activation='relu')(x) # A fully connected layer\n",
        "x = Dropout(0.5)(x) # Dropout for regularization to prevent overfitting\n",
        "predictions = Dense(num_classes, activation='softmax')(x) # Output layer with softmax for multi-class classification\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "# Using Adam optimizer, categorical_crossentropy for multi-class classification, and accuracy as metric\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JnQqkglpMdpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "within an MLflow run, logging parameters and using callbacks for model checkpointing, early stopping, and learning rate reduction."
      ],
      "metadata": {
        "id": "CNxR_LV29fbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train the model within an MLflow run ---\n",
        "with mlflow.start_run(run_name=\"MobileNetV2_TransferLearning\") as run:\n",
        "    # Log custom parameters (optional, autologging handles many)\n",
        "    mlflow.log_param(\"epochs\", 50)\n",
        "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "    mlflow.log_param(\"optimizer\", \"Adam\")\n",
        "    mlflow.log_param(\"learning_rate\", 0.001)\n",
        "    mlflow.log_param(\"base_model\", \"MobileNetV2\")\n",
        "\n",
        "    # Define callbacks (ModelCheckpoint is good for saving best model locally)\n",
        "    checkpoint_filepath = 'best_model.keras'\n",
        "    model_checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath, save_weights_only=False, monitor='val_accuracy',\n",
        "        mode='max', save_best_only=True, verbose=1\n",
        "    )\n",
        "    early_stopping_callback = EarlyStopping(\n",
        "        monitor='val_loss', patience=10, mode='min', verbose=1, restore_best_weights=True\n",
        "    )\n",
        "    reduce_lr_callback = ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1\n",
        "    )\n",
        "\n",
        "    EPOCHS = 50\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "        callbacks=[model_checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
        "    )\n",
        "\n",
        "    print(\"\\nModel training complete.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rQlCYUcGQxKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log the trained model, Evaluate it on the test set\n",
        "\n",
        "log test metrics, generate and log the classification report and confusion matrix as artifacts, and log plots of the training history to MLflow."
      ],
      "metadata": {
        "id": "SGVBwONQ9t85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"MobileNetV2_TransferLearning\") as run:\n",
        "    # Here, load the best model (saved by checkpoint) and log it to MLflow\n",
        "    best_model_for_mlflow = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    mlflow.keras.log_model(\n",
        "        best_model_for_mlflow,\n",
        "        artifact_path=\"apple_disease_model\",\n",
        "        registered_model_name=\"AppleDiseaseDetector\" # This registers it in the Model Registry\n",
        "    )\n",
        "\n",
        "    # Log evaluation metrics from the test set\n",
        "    print(\"\\nEvaluating the model on the test set for MLflow logging...\")\n",
        "    # Load the best model saved during training for evaluation\n",
        "    loaded_best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    eval_results = loaded_best_model.evaluate(\n",
        "        test_generator,\n",
        "        steps=test_generator.samples // test_generator.batch_size,\n",
        "        verbose=0\n",
        "    )\n",
        "    mlflow.log_metric(\"test_loss\", eval_results[0])\n",
        "    mlflow.log_metric(\"test_accuracy\", eval_results[1])\n",
        "\n",
        "    print(f\"Logged Test Loss: {eval_results[0]:.4f}\")\n",
        "    print(f\"Logged Test Accuracy: {eval_results[1]:.4f}\")\n",
        "\n",
        "    # Generate and log classification report and confusion matrix as artifacts\n",
        "    test_generator.reset()\n",
        "    class_labels = list(test_generator.class_indices.keys())\n",
        "    predictions = loaded_best_model.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    true_classes = test_generator.classes[:len(predicted_classes)]\n",
        "\n",
        "    # Get the unique true labels and sort them to use as labels for confusion matrix and classification report\n",
        "    unique_true_labels = sorted(np.unique(true_classes))\n",
        "\n",
        "    report = classification_report(true_classes, predicted_classes, target_names=class_labels, output_dict=True, labels=unique_true_labels)\n",
        "    mlflow.log_dict(report, \"classification_report.json\")\n",
        "\n",
        "    cm = confusion_matrix(true_classes, predicted_classes, labels=unique_true_labels)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[class_labels[i] for i in unique_true_labels], yticklabels=[class_labels[i] for i in unique_true_labels])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "    mlflow.log_artifact(\"confusion_matrix.png\")\n",
        "    plt.close() # Close plot to free memory\n",
        "\n",
        "    # Log plots of training history as artifacts\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_history.png\")\n",
        "    mlflow.log_artifact(\"training_history.png\")\n",
        "    plt.close() # Close plot\n",
        "\n",
        "    print(f\"MLflow Run ID: {run.info.run_id}\")\n",
        "    print(f\"MLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
      ],
      "metadata": {
        "id": "l20TvdET2Zv3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}